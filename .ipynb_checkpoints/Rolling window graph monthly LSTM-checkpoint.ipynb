{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "65caa597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "#import tensorflow.keras_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b343a817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17069934669649594892\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4046163968\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1487786962915463978\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d13a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data = pd.read_csv(\"./data/Bitcoin Price Monthly 2022 to 2010.csv\")\n",
    "graph_parameters= pd.read_csv(\"./data/monthlyparameters.csv\")\n",
    "scaler = StandardScaler()\n",
    "graph_parameters = pd.DataFrame(scaler.fit_transform(graph_parameters))\n",
    "market = market_data.iloc[50:50+graph_parameters.shape[0],1:].reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaeb9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df= pd.concat([market,graph_parameters],axis=1).iloc[:116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "596a837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=3\n",
    "def y_creator(threshold):\n",
    "    Y = []\n",
    "    for i in full_df[\"Change %\"][5:]:\n",
    "        if abs(i)>threshold:\n",
    "            if i>0:\n",
    "                Y.append(2)\n",
    "            else:\n",
    "                Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return Y\n",
    "Y = y_creator(threshold)\n",
    "y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce488a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\senior\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\anaconda3\\envs\\senior\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sc= MinMaxScaler(feature_range = (0 , 1))\n",
    "training_set_scaled =sc.fit_transform(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ddf6bf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Change %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337.9</td>\n",
       "      <td>388.2</td>\n",
       "      <td>407.7</td>\n",
       "      <td>294.9</td>\n",
       "      <td>569000.0</td>\n",
       "      <td>-12.96</td>\n",
       "      <td>0.867741</td>\n",
       "      <td>-1.094308</td>\n",
       "      <td>0.815107</td>\n",
       "      <td>-1.047186</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.888018</td>\n",
       "      <td>-2.851164</td>\n",
       "      <td>-2.946254</td>\n",
       "      <td>-2.892772</td>\n",
       "      <td>-2.877959</td>\n",
       "      <td>-2.784526</td>\n",
       "      <td>-2.830760</td>\n",
       "      <td>-2.908763</td>\n",
       "      <td>-2.811451</td>\n",
       "      <td>-2.904310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>374.9</td>\n",
       "      <td>337.9</td>\n",
       "      <td>480.5</td>\n",
       "      <td>319.8</td>\n",
       "      <td>711110.0</td>\n",
       "      <td>10.97</td>\n",
       "      <td>0.367717</td>\n",
       "      <td>-1.308259</td>\n",
       "      <td>0.039762</td>\n",
       "      <td>-0.733051</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.784331</td>\n",
       "      <td>-2.846469</td>\n",
       "      <td>-2.725048</td>\n",
       "      <td>-2.808438</td>\n",
       "      <td>-2.902495</td>\n",
       "      <td>-2.769891</td>\n",
       "      <td>-2.584454</td>\n",
       "      <td>-2.710144</td>\n",
       "      <td>-2.506208</td>\n",
       "      <td>-2.716775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>318.2</td>\n",
       "      <td>374.9</td>\n",
       "      <td>384.9</td>\n",
       "      <td>303.4</td>\n",
       "      <td>847290.0</td>\n",
       "      <td>-15.12</td>\n",
       "      <td>0.322376</td>\n",
       "      <td>-1.422352</td>\n",
       "      <td>0.341041</td>\n",
       "      <td>-0.892256</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.774707</td>\n",
       "      <td>-2.842872</td>\n",
       "      <td>-2.815712</td>\n",
       "      <td>-2.666033</td>\n",
       "      <td>-2.874102</td>\n",
       "      <td>-2.910996</td>\n",
       "      <td>-2.684058</td>\n",
       "      <td>-2.875429</td>\n",
       "      <td>-2.876391</td>\n",
       "      <td>-2.703630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>218.5</td>\n",
       "      <td>318.2</td>\n",
       "      <td>321.4</td>\n",
       "      <td>157.3</td>\n",
       "      <td>1560000.0</td>\n",
       "      <td>-31.34</td>\n",
       "      <td>0.148711</td>\n",
       "      <td>-0.312390</td>\n",
       "      <td>0.491503</td>\n",
       "      <td>-0.414691</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.666927</td>\n",
       "      <td>-2.648954</td>\n",
       "      <td>-2.527208</td>\n",
       "      <td>-2.519428</td>\n",
       "      <td>-2.300349</td>\n",
       "      <td>-2.629721</td>\n",
       "      <td>-2.075267</td>\n",
       "      <td>-2.374899</td>\n",
       "      <td>-2.730673</td>\n",
       "      <td>-2.703933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254.1</td>\n",
       "      <td>218.5</td>\n",
       "      <td>264.6</td>\n",
       "      <td>209.7</td>\n",
       "      <td>2090000.0</td>\n",
       "      <td>16.27</td>\n",
       "      <td>0.322061</td>\n",
       "      <td>-1.400236</td>\n",
       "      <td>0.141775</td>\n",
       "      <td>-0.333127</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.576023</td>\n",
       "      <td>-2.627471</td>\n",
       "      <td>-2.803466</td>\n",
       "      <td>-2.345500</td>\n",
       "      <td>-2.589167</td>\n",
       "      <td>-2.708548</td>\n",
       "      <td>-2.408913</td>\n",
       "      <td>-2.749788</td>\n",
       "      <td>-2.550230</td>\n",
       "      <td>-2.430377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>37298.6</td>\n",
       "      <td>57719.1</td>\n",
       "      <td>59523.9</td>\n",
       "      <td>30261.7</td>\n",
       "      <td>5330000.0</td>\n",
       "      <td>-35.38</td>\n",
       "      <td>0.855261</td>\n",
       "      <td>0.185272</td>\n",
       "      <td>-0.090300</td>\n",
       "      <td>0.880299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483062</td>\n",
       "      <td>0.498976</td>\n",
       "      <td>0.498072</td>\n",
       "      <td>0.536945</td>\n",
       "      <td>0.522401</td>\n",
       "      <td>0.391497</td>\n",
       "      <td>0.602725</td>\n",
       "      <td>0.532478</td>\n",
       "      <td>0.527842</td>\n",
       "      <td>0.486704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>35026.9</td>\n",
       "      <td>37294.3</td>\n",
       "      <td>41318.0</td>\n",
       "      <td>28901.8</td>\n",
       "      <td>4140000.0</td>\n",
       "      <td>-6.09</td>\n",
       "      <td>0.701507</td>\n",
       "      <td>0.773294</td>\n",
       "      <td>0.063339</td>\n",
       "      <td>0.816858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408561</td>\n",
       "      <td>0.528576</td>\n",
       "      <td>0.510254</td>\n",
       "      <td>0.548455</td>\n",
       "      <td>0.558271</td>\n",
       "      <td>0.539322</td>\n",
       "      <td>0.558289</td>\n",
       "      <td>0.535049</td>\n",
       "      <td>0.513040</td>\n",
       "      <td>0.393888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>41553.7</td>\n",
       "      <td>35030.7</td>\n",
       "      <td>42285.3</td>\n",
       "      <td>29310.2</td>\n",
       "      <td>2440000.0</td>\n",
       "      <td>18.63</td>\n",
       "      <td>-0.197871</td>\n",
       "      <td>0.431599</td>\n",
       "      <td>-0.550930</td>\n",
       "      <td>1.253711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394067</td>\n",
       "      <td>0.539134</td>\n",
       "      <td>0.508395</td>\n",
       "      <td>0.542236</td>\n",
       "      <td>0.560960</td>\n",
       "      <td>0.515715</td>\n",
       "      <td>0.574107</td>\n",
       "      <td>0.517499</td>\n",
       "      <td>0.517778</td>\n",
       "      <td>0.367340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>47130.4</td>\n",
       "      <td>41510.0</td>\n",
       "      <td>50498.8</td>\n",
       "      <td>37365.4</td>\n",
       "      <td>2140000.0</td>\n",
       "      <td>13.42</td>\n",
       "      <td>-0.403262</td>\n",
       "      <td>0.615532</td>\n",
       "      <td>0.119538</td>\n",
       "      <td>1.396842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413361</td>\n",
       "      <td>0.516034</td>\n",
       "      <td>0.496518</td>\n",
       "      <td>0.535938</td>\n",
       "      <td>0.531789</td>\n",
       "      <td>0.481542</td>\n",
       "      <td>0.595415</td>\n",
       "      <td>0.556078</td>\n",
       "      <td>0.527477</td>\n",
       "      <td>0.448777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>43823.3</td>\n",
       "      <td>47129.2</td>\n",
       "      <td>52885.3</td>\n",
       "      <td>39646.8</td>\n",
       "      <td>2210000.0</td>\n",
       "      <td>-7.02</td>\n",
       "      <td>-0.415245</td>\n",
       "      <td>0.381490</td>\n",
       "      <td>-0.033737</td>\n",
       "      <td>1.294406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367240</td>\n",
       "      <td>0.526980</td>\n",
       "      <td>0.505076</td>\n",
       "      <td>0.540302</td>\n",
       "      <td>0.541316</td>\n",
       "      <td>0.484758</td>\n",
       "      <td>0.594034</td>\n",
       "      <td>0.551116</td>\n",
       "      <td>0.525831</td>\n",
       "      <td>0.449840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Price     Open     High      Low       Vol.  Change %         0  \\\n",
       "0     337.9    388.2    407.7    294.9   569000.0    -12.96  0.867741   \n",
       "1     374.9    337.9    480.5    319.8   711110.0     10.97  0.367717   \n",
       "2     318.2    374.9    384.9    303.4   847290.0    -15.12  0.322376   \n",
       "3     218.5    318.2    321.4    157.3  1560000.0    -31.34  0.148711   \n",
       "4     254.1    218.5    264.6    209.7  2090000.0     16.27  0.322061   \n",
       "..      ...      ...      ...      ...        ...       ...       ...   \n",
       "79  37298.6  57719.1  59523.9  30261.7  5330000.0    -35.38  0.855261   \n",
       "80  35026.9  37294.3  41318.0  28901.8  4140000.0     -6.09  0.701507   \n",
       "81  41553.7  35030.7  42285.3  29310.2  2440000.0     18.63 -0.197871   \n",
       "82  47130.4  41510.0  50498.8  37365.4  2140000.0     13.42 -0.403262   \n",
       "83  43823.3  47129.2  52885.3  39646.8  2210000.0     -7.02 -0.415245   \n",
       "\n",
       "           1         2         3  ...        30        31        32        33  \\\n",
       "0  -1.094308  0.815107 -1.047186  ... -2.888018 -2.851164 -2.946254 -2.892772   \n",
       "1  -1.308259  0.039762 -0.733051  ... -2.784331 -2.846469 -2.725048 -2.808438   \n",
       "2  -1.422352  0.341041 -0.892256  ... -2.774707 -2.842872 -2.815712 -2.666033   \n",
       "3  -0.312390  0.491503 -0.414691  ... -2.666927 -2.648954 -2.527208 -2.519428   \n",
       "4  -1.400236  0.141775 -0.333127  ... -2.576023 -2.627471 -2.803466 -2.345500   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "79  0.185272 -0.090300  0.880299  ...  0.483062  0.498976  0.498072  0.536945   \n",
       "80  0.773294  0.063339  0.816858  ...  0.408561  0.528576  0.510254  0.548455   \n",
       "81  0.431599 -0.550930  1.253711  ...  0.394067  0.539134  0.508395  0.542236   \n",
       "82  0.615532  0.119538  1.396842  ...  0.413361  0.516034  0.496518  0.535938   \n",
       "83  0.381490 -0.033737  1.294406  ...  0.367240  0.526980  0.505076  0.540302   \n",
       "\n",
       "          34        35        36        37        38        39  \n",
       "0  -2.877959 -2.784526 -2.830760 -2.908763 -2.811451 -2.904310  \n",
       "1  -2.902495 -2.769891 -2.584454 -2.710144 -2.506208 -2.716775  \n",
       "2  -2.874102 -2.910996 -2.684058 -2.875429 -2.876391 -2.703630  \n",
       "3  -2.300349 -2.629721 -2.075267 -2.374899 -2.730673 -2.703933  \n",
       "4  -2.589167 -2.708548 -2.408913 -2.749788 -2.550230 -2.430377  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "79  0.522401  0.391497  0.602725  0.532478  0.527842  0.486704  \n",
       "80  0.558271  0.539322  0.558289  0.535049  0.513040  0.393888  \n",
       "81  0.560960  0.515715  0.574107  0.517499  0.517778  0.367340  \n",
       "82  0.531789  0.481542  0.595415  0.556078  0.527477  0.448777  \n",
       "83  0.541316  0.484758  0.594034  0.551116  0.525831  0.449840  \n",
       "\n",
       "[84 rows x 46 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30981fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a368ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train =[]\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "for i in range (4,67):\n",
    "    X_train.append(np.array(training_set_scaled[i-4:i]).reshape(full_df.shape[1]*4,1))\n",
    "    y_train.append(y[i])\n",
    "\n",
    "for i in range (67,79):\n",
    "    X_test.append(np.array(training_set_scaled[i-4:i]).reshape(full_df.shape[1]*4,1))\n",
    "    y_test.append(y[i])\n",
    "\n",
    "X_train , y_train =np.array(X_train), np.array(y_train)\n",
    "X_test , y_test =np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9e79e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creator(X_train):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units =100  , return_sequences = True , input_shape = (X_train.shape[1] , 1)     ))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units = 50  , return_sequences = True  ))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units = 25 ))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units= 3,kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d2feb433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 6s 149ms/step - loss: 1.1467 - precision_3: 0.3598 - recall_3: 0.9365\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.0842 - precision_3: 0.4646 - recall_3: 0.9365\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.0445 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.9974 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.9663 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9534 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9458 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9491 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9530 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9543 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9252 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9334 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9344 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.9241 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9206 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9109 - precision_3: 0.4640 - recall_3: 0.9206\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9228 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9291 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9360 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9359 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9460 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9307 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9217 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9566 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9454 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9540 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9002 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9175 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9329 - precision_3: 0.4640 - recall_3: 0.9206\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9213 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9264 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9245 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9243 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9447 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9376 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9313 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.9426 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9509 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9296 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9267 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9061 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9335 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9253 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9301 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9148 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9334 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9317 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9371 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9126 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9097 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9281 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9192 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9223 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9170 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9216 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9153 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9160 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9506 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9122 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9195 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9243 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9346 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9266 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9167 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9251 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9242 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9203 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9094 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9172 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9198 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9395 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9315 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9214 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9065 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9330 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9299 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.8993 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9204 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9194 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.9188 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9243 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9224 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9181 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9244 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.9314 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.9243 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9204 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9149 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.8990 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9080 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.8914 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.8970 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9175 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9286 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9140 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9063 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9066 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9104 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9209 - precision_3: 0.4683 - recall_3: 0.9365\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9050 - precision_3: 0.4683 - recall_3: 0.9365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x234e751ad90>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_creator(X_train)\n",
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd2c3e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 137 calls to <function Model.make_test_function.<locals>.test_function at 0x00000234EBA9EAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Accuracy on training data: 0.4682539701461792% \n",
      " Error on training data: 0.5317460298538208\n",
      "Accuracy on test data: 0.4583333432674408% \n",
      " Error on test data: 0.5416666567325592\n"
     ]
    }
   ],
   "source": [
    "pred_train= model.predict(X_train)\n",
    "scores = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Accuracy on training data: {}% \\n Error on training data: {}'.format(scores[1], 1 - scores[1]))   \n",
    " \n",
    "pred_test= model.predict(X_test)\n",
    "scores2 = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy on test data: {}% \\n Error on test data: {}'.format(scores2[1], 1 - scores2[1]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "322f61c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 3],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 8]], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test.argmax(axis=1), pred_test.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "31648f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1f7aace7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, 26],\n",
       "       [ 0,  0,  4],\n",
       "       [ 0,  0, 33]], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train.argmax(axis=1), y_pred_train.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4d54d621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6924725 , 0.23372239, 0.73341316],\n",
       "       [0.692484  , 0.23369728, 0.7334427 ],\n",
       "       [0.6925195 , 0.23364738, 0.73347276],\n",
       "       [0.6924215 , 0.23377275, 0.73335826],\n",
       "       [0.6925227 , 0.23364279, 0.733485  ],\n",
       "       [0.6925913 , 0.23354538, 0.73355013],\n",
       "       [0.6925722 , 0.23358512, 0.73351794],\n",
       "       [0.6924835 , 0.23369421, 0.7334488 ],\n",
       "       [0.6924486 , 0.23376617, 0.7333591 ],\n",
       "       [0.6924683 , 0.2337494 , 0.73338264],\n",
       "       [0.6925988 , 0.23356555, 0.73350424],\n",
       "       [0.69256395, 0.23363107, 0.73345804]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555107c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
